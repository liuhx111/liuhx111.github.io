<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hangxin Liu</title>
  
  <meta name="author" content="Hangxin Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hangxin Liu</name>
              </p>
              <p style="text-align:justify;">I am a research scientist and team lead of the Robotics Lab at <a href="https://www.bigai.ai/">Beijing Institute for General Artificial Intelligence</a>. In the robotics lab, my colleagues and I hypothesize that there exists some fundamental representations or cognitive architectures underpinning the intelligent behaviors of humans. By uncovering and reproducing such an architecture, we hope to enable long-term human-robot shared autonomy by bridging
              </p>
              <ul>
                <li><strong>Perception</strong> (how to extract and organize more expressive symbols from dense sensory signals);
                <li><strong>Reasoning</strong> (how to utilze thoes abstract information for higher-level skills, which in turn faciliates perception);
                <li><strong>Task and Motion Planning</strong> (how to effectively act and react).
              </ul>
              <p style="text-align:justify;"> I received a Ph.D. in Computer Science and a M.S. in Mechanical Engineering from UCLA, working in the <a href="http://vcla.stat.ucla.edu/" target="_blank">Center for Vision, Cognition, Learning, and Autonomy</a> with <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Professor Song-Chun Zhu</a>. My work at UCLA was supported by DARPA SIMPLEX, DARPA XAI, ONR MURI, and ONR Cognitive Robot.</p>
                                        
              <p style="text-align:justify;">Before joining VCLA, I graduated with a B.S. in Mechanical Engineering and a B.S. in Computer Science with a Mathematics minor from Virginia Polytechnic Institute and State University (Virginia Tech) in 2016.
              </p>
              <p style="text-align:center">
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1D5tDu0AAAAJ&hl=en&oi=ao">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/hx_liu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/hx_liu_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in robot perception, planning, learning, human-robot interaction, and virtual and augmented reality. 
                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/23_part.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Part-Level Scene Reconstruction Affords Robot Interaction</papertitle>
              </a>
              <br>
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a>Lexing Zhang</a>*,
              <a>Zaijin Wang</a>,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>,
              <a href="https://sites.google.com/view/muzhihan/home">Muzhi Han</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
              <br>
              <a href="">Paper</a> / 
              <a href="">Video</a> /
              <a href="">Project Page</a>
              <p></p>
              <p>
              We extend our work in IJCV22 and ICRA21 by segmenting the objects in the panoptic map into parts and replace those parts by primitive shapes, which results in more realistic functionally equivalent scenes.
              </p>
            </td>
          </tr>
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/23_cut.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Learning a Causal Transition Model for Object Cutting</papertitle>
              </a>
              <br>
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a href="https://sites.google.com/view/muzhihan/home">Muzhi Han</a>*,
              <a href="https://buzz-beater.github.io/">Baoxiong Jia</a>,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
              <br>
              <a href="">Paper</a> / 
              <a href="">Video</a> /
              <a href="">Project Page</a>
              <p></p>
              <p>
              An attributed stochastic grammar is proposed to model the process of object fragmentation during cutting, which abstracts the spatial arrangement of fragments as node variables and captures the causality of cutting actions based on the fragmentation of parts.
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/23_wmr.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Aggregating Single-wheeled Mobile Robots for Omnidirectional Movements</papertitle>
              </a>
              <br>
              <a href="https://meng-wang.jimdofree.com/">Meng Wang*</a>,
              <a href="https://yaosu.info/">Yao Su*</a>,
              <a>Hang Li</a>,
              <a href="https://jrli.org/">Jiarui Li</a>,
              <a>Jixiang Liang</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
              <br>
              <a href="">Paper</a> / 
              <a href="">Video</a> /
              <a href="">Project Page</a>
              <p></p>
              <p>
              We present a novel modular robot system capable of self-reconfiguration and achieving omnidirectional movements through magnetic docking for collaborative object transportation. Each robot in the system only equips a steerable omni wheel for navigation.
              </p>
            </td>
          </tr>
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/23_uam.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Sequential Manipulation Planning for Over-actuated Uumanned Aerial Manipulators</papertitle>
              </a>
              <br>
              <a href="https://yaosu.info/">Yao Su*</a>,
              <a href="https://jrli.org/">Jiarui Li*</a>,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>*,
              <a href="https://meng-wang.jimdofree.com/">Meng Wang</a>,
              <a>Chi Chu</a>,
              <a>Hang Li</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
              <br>
              <a href="">Paper</a> / 
              <a href="">Video</a> /
              <a href="https://marvel-uav.github.io/">Project Page</a>
              <p></p>
              <p>
              Instead of one-step aerial manipulation tasks, we investigate the sequential manipulation planning problem of UAMs, which requires coordinated motions of the vehicleâ€™s floating base, the manipulator, and the object being manipulated over a long horizon.
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/23_L3FT.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>L3 F-TOUCH: A Wireless GelSight with Decoupled Tactile and Three-axis Force Sensing</papertitle>
              </a>
              <br>
              <a href="">Wanlin Li</a>*,
              <a href="https://meng-wang.jimdofree.com/">Meng Wang</a>*,
              <a href="https://jrli.org/">Jiarui Li</a>,
              <a href="https://yaosu.info/">Yao Su</a><span>&#9993,
              <a>Devesh K. Jha</a>,
              <a>Xinyuan Qian</a>,
              <a>Kaspar Althoefer</a>,
              <strong>Hangxin Liu</strong><span>&#9993
              <br>* equal contributors
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2023
              <br>
              <a href="data/paper/RAL23_L3F_T.pdf">Paper</a> / 
              <a href="">Video</a> /
              <a href="">Project Page</a>
              <p></p>
              <p>
              We present an L3 F-TOUCH sensor that considerably enhances the three-axis force sensing capability of typical GelSight sensors, while being Lightweight, Low-cost, and supporting wireLess deplyment.
              </p>
            </td>
          </tr>
          
        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/23_glove.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps</papertitle>
              </a>
              <br>
              <strong>Hangxin Liu*</strong><span>&#9993,
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>*,
              <a href="https://www.zlz.link/">Zhenliang Zhang</a>,
              <a href="https://www.math.ucla.edu/~minchen/">Minchen Li</a>,
              <a href="https://www.math.ucla.edu/~cffjiang/">Chenfanfu Jiang</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a><span>&#9993;</span>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>* equal contributors
              <br>
              <em>Engineering</em>, 2023
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S2095809923000978">Paper</a> /
              <a href="">Video</a> /
              <a href="">Project Page</a>
              <p></p>
              <p>To endow embodied AI agents with a deeper understanding of hand-object interactions, we design a data glove that can be reconfigured to collect grasping data in three modes: (i) force exerted by hand using piezoresistive material, (ii) contact points by grasping stably in VR, and (iii) reconstruct both visual and physical effects during the manipulation by integrating physics-based simulation.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/23_scene_redesign.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Rearrange Indoor Scenes for Human-Robot Co-Activity</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/rockywang/home">Weiqi Wang</a>*,
              <a>Zihang Zhao</a>*,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>*,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023
              <br>
              <a href="">Paper</a> / 
              <a href="">Video</a> /
              <a href="">Project Page</a>
              <p></p>
              <p>
              We present an optimization framework to redesign an indoor scene by rearranging the furniture within it, which maximizes free space for service robots to operate while preserving human's preference for scene layout.
              </p>
            </td>
          </tr>
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/22_scene.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Scene Reconstruction with Functional Objects for Robot Autonomy</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/muzhihan/home">Muzhi Han</a>*,
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>,
              <a href="https://xuxie1031.github.io/">Xu Xie</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a><span>&#9993;</span>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <strong>Hangxin Liu</strong><span>&#9993;</span>
              <br>* equal contributors
              <br>
              <em>International Journal of Computer Vision (IJCV)</em>, 2022
              <br>
              <a href="data/paper/IJCV22_scene.pdf">Paper</a> /
              <a href="https://sites.google.com/view/ijcv2022-reconstruction">Project Page</a>
              <p></p>
              <p>We rethink the problem of scene reconstruction from an embodied agentâ€™s perspective. The objects within a reconstructed scene are segmented and replaced by part-based articulated CAD models to provide actionable information and afford finer-grained robot interactions.</p>
            </td>
          </tr> 
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/22_ged.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Sequential Manipulation Planning on Scene Graph</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>,
              <a href="">Yida Niu</a>,
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/index.html">Song-Chun Zhu</a>, 
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
              <br>
              <a href="data/paper/IROS22_GED.pdf">Paper</a> / 
              <a href="https://vimeo.com/728421779">Video</a> /
              <a href="https://sites.google.com/view/planning-on-graph">Project Page</a>
              <p></p>
              <p>
              We devise a 3D scene graph representation to abstract scene layouts with succinct geometric information and valid robot-scene interactions, such that a valid task plan can be computed using graph editing distance between the initial and the final scene graph while effectively satisfying constraints in motion level.
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/22_uav.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Downwash-aware Control Allocation for Over-actuated UAV Platforms</papertitle>
              </a>
              <br>
              <a href="https://yaosu.info/">Yao Su*</a>,
              <a>Chi Chu*</a>,
              <a href="https://meng-wang.jimdofree.com/">Meng Wang</a>,
              <a>Jiarui Li</a>,
              <a>Liu Yang</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
              <br>
              <a href="data/paper/IROS22_Downwash.pdf">Paper</a> / 
              <a href="https://vimeo.com/729266370">Video</a> /
              <a href="https://marvel-uav.github.io/">Project Page</a>
              <p></p>
              <p>
              Leveraging the input redundancy in over-actuated UAVs, we tackle downwash effects between propellers with a novel control allocation framework that explores the entire allocation space for an optimal solution that reduces counteractions of airflows.
              </p>
            </td>
          </tr>


          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/22_tool.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Understanding Physical Effects for Effective Tool-use</papertitle>
              </a>
              <br>
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>*,
              <a href="">Weiqi Wang</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/index.html">Song-Chun Zhu</a>,
              <strong>Hangxin Liu</strong><span>&#9993;</span>
              <br>* equal contributors
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L+IROS)</em>, 2022
              <br>
              <a href="data/paper/RAL22_ToolUse.pdf">Paper</a> / 
              <a href="https://vimeo.com/725191188">Video</a> 
              <p></p>
              <p>
              Learning key physical properties of tool-uses from a FEM-based simulation and enacting those properties via an optimal control-based motion planning scheme to produce tool-use strategies drastically different from observations, but with the least joint efforts.
              </p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/22_duo.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Object Gathering with a Tethered Robot Duo</papertitle>
              </a>
              <br>
              <a href="https://yaosu.info/">Yao Su</a>,
              <a>Yuhong Jiang</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <strong>Hangxin Liu</strong><span>&#9993;</span>
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L+ICRA)</em>, 2022
              <br>
              <a href="data/paper/RAL22_Tethered_Duo.pdf">Paper</a> / 
              <a href="https://player.vimeo.com/video/662175853?color=ff9933&byline=0&portrait=0">Video</a> /
              <a href="https://yaosu.info/papers/2021-ral-tetherduo/index.html">Project Page</a>
              <p></p>
              <p>
              A cooperative planning framework to generate optimal trajectories for a robot duo tethered by a flexible net to gather scattered objects spread in a large area. Implemented Model Reference Adaptive Control (MRAC) to handle unknown dynamics of carried payloads.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/21_aail_ar.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Patching Interpretable And-Or-Graph Knowledge Representation using Augmented Reality</papertitle>
              </a>
              <br>
              <strong>Hangxin Liu</strong>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/index.html">Song-Chun Zhu</a>
              <br>
              <em>Applied AI Letters</em>, 2021 &nbsp <font color="red"><strong>(DARPA XAI Speical Issue)</strong></font>
              <br>
              <a href="https://onlinelibrary.wiley.com/doi/10.1002/ail2.43">Paper</a>
              <p></p>
              <p>
              Given an interpretable And-Or-Graph knowledge representation, the proposed AR interface allows users to intuitively understand and supervise robot's behaviors, and interactively teach the robot with new actions.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video width=100% height=100% muted autoplay loop>
                <source src="images/21_vkc_motion_iros.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Consolidating Kinematic Models to Promote Coordinated Mobile Manipulations</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>*,
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a href="">Xin Jiang</a>,
              <a href="https://drexel.edu/engineering/about/faculty-staff/H/han-david/">David Han</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021
              <br>
              <a href="data/paper/IROS21_OpenDoor_Motion.pdf">Paper</a> /
              <a href="https://vimeo.com/581565256">Video</a> /
              <a href="https://github.com/zyjiao4728/Planning-on-VKC">Code</a>
              <p></p>
              <p>Constructing a Virtual Kinematic Chain (VKC) that readily consolidates the kinematics of the mobile base, the arm, and the object to be manipulated in mobile manipulations.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/21_vkc_task_iros.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Efficient Task Planning for Mobile Manipulation: a Virtual Kinematic Chain Perspective</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>*,
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a href="">Weiqi Wang</a>,
              <a href="https://drexel.edu/engineering/about/faculty-staff/H/han-david/">David Han</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021
              <br>
              <a href="data/paper/IROS21_VKC_Task.pdf">Paper</a> /
              <a href="https://vimeo.com/581563536">Video</a> /
              <a href="https://github.com/TooSchoolForCool/IROS21-VKC-Task-PDDL">Code</a>
              <p></p>
              <p>The VKC perspective is a simple yet effective method to improve task planning efficacy for mobile manipulation.</p>
            </td>
          </tr> 

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/21_scene_icra.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model Alignments</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/muzhihan/home">Muzhi Han</a>*,
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>*,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>,
              <a href="https://xuxie1031.github.io/">Xu Xie</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <strong>Hangxin Liu</strong>
              <br>* equal contributors
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2021
              <br>
              <a href="data/paper/ICRA21_scene.pdf">Paper</a> /
              <a href="https://vimeo.com/530222887">Video</a> /
              <a href="https://github.com/hmz-15/Interactive-Scene-Reconstruction">Code</a>
              <p></p>
              <p>Reconstructing an interactive scene from RGB-D data stream by panoptic mapping and organizing object affordance and contextual relations by a graph-based scene representation.</p>
            </td>
          </tr> 
      
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/20_workspace_iros.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Human-Robot Interaction in a Shared Augmented Reality Workspace</papertitle>
              </a>
              <br>
              <a href="https://janetalready.github.io/">Shuwen Qiu</a>*,
              <strong>Hangxin Liu</strong>*,
              <a href="https://tooschoolforcool.github.io/">Zeyu Zhang</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2020
              <br>
              <a href="data/paper/IROS20_Workspace.pdf">Paper</a> /
              <a href="https://vimeo.com/439150333">Video</a> /
              <a href="https://github.com/Janetalready/Shared-AR-workspace">Code</a> /
              <a href="https://vimeo.com/451048919">Presentation</a>
              <p></p>
              <p>Physical robots can control and alter virtual objects in AR as an active agent and proactively interact with human agents, instead of purely passively executing received commands.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/20_walkingbot_roman.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>WalkingBot: Modular Interactive Legged Robot with Automated Structure Sensing and Motion Planning</papertitle>
              </a>
              <br>
              <a href="https://meng-wang.jimdofree.com/">Meng Wang</a>,
              <a href="">Yao Su</a>,
              <strong>Hangxin Liu</strong>,
              <a href="">Yingqing Xu</a>
              <br>
              <em>IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>, 2020
              <br>
              <a href="data/paper/ROMAN20_Walkingbot.pdf">Paper</a>
              <p></p>
              <p>A modular robot system that allows non-expert users to build a multi-legged robot in various morphologies using a set of building blocks with sensors and actuators embedded.</p>
            </td>
          </tr> 

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/20_tom_icra.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs</papertitle>
              </a>
              <br>
              <a href="">Tao Yuan</a>,
              <strong>Hangxin Liu</strong>,
              <a href="https://lifengfan.github.io/">Lifeng Fan</a>, 
              <a href="http://web.cs.ucla.edu/~zilongzheng/">Zilong Zheng</a>, 
              <a href="http://www.stat.ucla.edu/~taogao/">Tao Gao</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020
              <br>
              <a href="data/paper/ICRA20_ToM.pdf">Paper</a> /
              <a href="https://vimeo.com/391734593">Video</a> /
              <a href="https://vimeo.com/420949549">Presentation</a>
              <p></p>
              <p>A graphical model to unify the representation of object states, robot knowledge, and human (false-)beliefs.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/20_evacuation.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Congestion-aware Evacuation Routing using Augmented Reality Devices</papertitle>
              </a>
              <br>
              <a>Zeyu Zhang</a>,
              <strong>Hangxin Liu</strong>,
              <a href="https://sites.google.com/g.ucla.edu/zyjiao">Ziyuan Jiao</a>,
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020
              <br>
              <a href="data/paper/ICRA20_Evacuation.pdf">Paper</a> /
              <a href="https://vimeo.com/391765127">Video</a> /
              <a href="https://github.com/TooSchoolForCool/AR-Evacuation-ICRA">Code</a> /
              <a href="https://vimeo.com/420833403">Presentation</a>
              <p></p>
              <p>
              An AR-based indoor evacuation system with a congestion-aware routing solution. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/20_dark.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Human-like Commonsense</papertitle>
              </a>
              <br>
              <a href="https://yzhu.io/">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~taogao/">Tao Gao</a>,
              <a href="https://lifengfan.github.io/">Lifeng Fan</a>,
              <a href="https://siyuanhuang.com/">Siyuan Huang</a>,
              <a href="http://www.mjedmonds.com/">Mark Edmonds</a>,
              <strong>Hangxin Liu</strong>,
              <a href="https://fen9.github.io/">Feng Gao</a>,
              <a href="http://wellyzhang.github.io/">Chi Zhang</a>, 
              <a href="http://web.cs.ucla.edu/~syqi/">Siyuan Qi</a>,
              <a href="">Ying Nian Wu</a>,
              <a href="https://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>,   
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>
              <em>Engineering</em>, 2020
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S2095809920300345">Paper</a>
              <p></p>
              <p>
              A comprehensive review on cognitive AI and visual commonsense (Functionality, Physics, Intension, Causality).  
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/19_scirob.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>A tale of two explanations: Enhancing human trust by explaining robot behavior</papertitle>
              </a>
              <br>
              <a href="http://www.mjedmonds.com/">Mark Edmonds</a>*,
              <a href="https://fen9.github.io">Feng Gao</a>*,
              <strong>Hangxin Liu*</strong>,
              <a href="https://xuxie1031.github.io/">Xu Xie*</a>,
              <a href="http://web.cs.ucla.edu/~syqi/">Siyuan Qi</a>,
              <a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock">Brandon Rothrock</a>,
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="">Ying Nian Wu</a>,
              <a href="http://cvl.psych.ucla.edu/">Hongjing Lu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>* equal contributors
              <br>
              <em>Science Robotics</em>, 2019
              <br>
              <a href="https://www.science.org/doi/10.1126/scirobotics.aay4663">Paper</a>
              <p></p>
              <p>
              Learning multi-modal knowledge representation and fostering human trusts by producing explanations. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/19_vrgym.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>VRGym: A Virtual Testbed for Physical and Interactive AI</papertitle>
              </a>
              <br>
              <a href="https://xuxie1031.github.io/">Xu Xie</a>, 
              <strong>Hangxin Liu</strong>,
              <a href="http://www.zlz.link/">Zhenliang Zhang</a>, 
              <a href="https://yuxingqiu.github.io/">Yuxing Qiu</a>, 
              <a href="https://fen9.github.io/">Feng Gao</a>, 
              <a href="http://web.cs.ucla.edu/~syqi/">Siyuan Qi</a>, 
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>
              <em>ACM Turing Celebration Conference - China (ACM TURC)</em>, 2019
              <br>
              <a href="data/paper/TURC19_VRGym.pdf">Paper</a> / 
              <a href="https://vimeo.com/327629749">Video</a> /
              <a href="https://gitlab.com/vcla/vrgym">Code</a>
              <p></p>
              <p>
              A testbed with fine-grained physical effects and realistic human-robot interactions for training embodied agents. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/19_ssl.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Self-Supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment</papertitle>
              </a>
              <br> 
              <strong>Hangxin Liu</strong>*,
              <a>Zeyu Zhang</a>*, 
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>* equal contributors
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2019
              <br>
              <a href="data/paper/ICRA19_SSL.pdf">Paper</a> / 
              <a href="https://vimeo.com/321150838">Video</a> /
              <a href="https://github.com/TooSchoolForCool/EddieBot-ROS">Code</a>
              <p></p>
              <p>
              Robot 'labels' received data by its own exploration and refines its predictive model on-the-fly. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">.
                <img src='images/19_vrglove.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>High-Fidelity Grasping in Virtual Reality using a Glove-based System</papertitle>
              </a>
              <br> 
              <strong>Hangxin Liu</strong>*,
              <a href="http://www.zlz.link/">Zhenliang Zhang</a>*,
              <a href="https://xuxie1031.github.io/">Xu Xie</a>, 
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="#">Yue Liu</a>,
              <a href="#">Yongtian Wang</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>* equal contributors
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2019
              <br>
              <a href="data/paper/ICRA19_VRGlove.pdf">Paper</a> / 
              <a href="https://vimeo.com/317890470">Video</a> /
              <a href="https://github.com/zzlyw/ICRA19_VRGloveSystem">Code</a>
              <p></p>
              <p>
              A data glove for natural human grasping activities in VR and for cost-effective grasping data collections. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/19_mirror.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Mirroring without Overimitation: Learning Functionally Equivalent Manipulation Actions</papertitle>
              </a>
              <br> 
              <strong>Hangxin Liu</strong>,
              <a href="http://wellyzhang.github.io/">Chi Zhang</a>, 
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="https://www.seas.upenn.edu/~cffjiang/">Chenfanfu Jiang</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>
              <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2019
              <br>
              <a href="data/paper/AAAI19_Mirroring.pdf">Paper</a>
              <p></p>
              <p>
              Learning functionally equivalent actions that produce similar effects instead of learning trajectory cues. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/18_patching.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Interactive Robot Knowledge Patching using Augmented Reality</papertitle>
              </a>
              <br> 
              <strong>Hangxin Liu</strong>*,
              <a href="#">Yaofang Zhang</a>*,
              <a href="#">Wenwen Si</a>,
              <a href="https://xuxie1031.github.io/">Xu Xie</a>,  
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
              <br>* equal contributors
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2018
              <br>
              <a href="data/paper/icra18_arpatching.pdf">Paper</a> / 
              <a href="https://vimeo.com/256261673">Video</a> /
              <a href="https://github.com/xiaozhuchacha/AOG_AR">Code</a>
              <p></p>
              <p>
              An AR system for users to diagnose robor's problems, correct wrong behaviors, and add the corrections to the robot's knowledge. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/18_gloveaction.webp' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Unsupervised Learning using Hierarchical Models for Hand-Object Interactions</papertitle>
              </a>
              <br> 
              <a href="https://xuxie1031.github.io/">Xu Xie</a>*,
              <strong>Hangxin Liu</strong>*,
              <a href="http://www.mjedmonds.com/">Mark Edmonds</a>,
              <a href="https://fen9.github.io">Feng Gao</a>,
              <a href="http://web.cs.ucla.edu/~syqi/">Siyuan Qi</a>,
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock">Brandon Rothrock</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>.
              <br>* equal contributors
              <br>
              <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2018
              <br>
              <a href="data/paper/icra18_gloveaction.pdf">Paper</a> / 
              <a href="https://vimeo.com/259416784">Video</a> /
              <a href="https://github.com/xuxie1031/UnsupervisedGloveAction">Code</a>
              <p></p>
              <p>
              An unsupervised learning approach for manipulation event segmentation and parsing using hand gestures and forces data.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/17_glove.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>A Glove-based System for Studying Hand-Object Manipulation via Joint Pose and Force Sensing</papertitle>
              </a>
              <br> 
              <strong>Hangxin Liu</strong>*,
              <a href="https://xuxie1031.github.io/">Xu Xie</a>*,
              <a href="#">Matt Millar</a>*,
              <a href="http://www.mjedmonds.com/">Mark Edmonds</a>,
              <a href="https://fen9.github.io">Feng Gao</a>,
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="http://www.mae.ucla.edu/veronica-santos/">Veronica Santos</a>,
              <a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock">Brandon Rothrock</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>.
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2017
              <br>
              <a href="data/paper/iros17_glove.pdf">Paper</a> / 
              <a href="https://github.com/xiaozhuchacha/VCLATactileGlove">Code</a>
              <p></p>
              <p>
              A easy-to-replicate glove-based system for real time collections of hand pose and force during fine manipulative actions.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/17_open_bottle.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles</papertitle>
              </a>
              <br> 
              <a href="http://www.mjedmonds.com/">Mark Edmonds</a>*,
              <a href="https://fen9.github.io">Feng Gao</a>*,
              <a href="https://xuxie1031.github.io/">Xu Xie</a>,
              <strong>Hangxin Liu</strong>,
              <a href="http://web.cs.ucla.edu/~syqi/">Siyuan Qi</a>,
              <a href="https://www.yzhu.io">Yixin Zhu</a>,
              <a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock">Brandon Rothrock</a>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>.
              <br>* equal contributors
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2017
              <br>
              <a href="data/paper/iros17_openbottle.pdf">Paper</a> / 
              <a href="https://vimeo.com/227504384">Video</a> /
              <a href="https://github.com/xiaozhuchacha/OpenBottle">Code</a>
              <p></p>
              <p>
              Learning an action planner through both a top-down stochastic grammar model (And-Or graph) and a bottom-up discriminative model from the observed poses and forces 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/16_traf_sensor.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Reliable Infrastructural Urban Traffic Monitoring Via Lidar and Camera Fusion</papertitle>
              </a>
              <br> 
              <a href="#">Yi Tian</a>,
              <strong>Hangxin Liu</strong>,
              <a href="#">Tomonari Furukawa</a>.
              <br>
              <em>SAE International Journal of Passenger Cars-Electronic and Electrical Systems</em>, 2017
              <br>
              <a href="https://go.gale.com/ps/i.do?id=GALE%7CA528961291&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=19464614&p=AONE&sw=w&userGroupName=anon%7E3321c0d1">Paper</a>
              <p></p>
              <p>
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/16_nfov_iros.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Non-Field-Of-View Sound Source Localization Using Diffraction and Reflection Signals</papertitle>
              </a>
              <br> 
              <a href="#">Kuya Takami</a>,
              <strong>Hangxin Liu</strong>,
              <a href="#">Tomonari Furukawa</a>,
              <a href="#">Makoto Kumon</a>,
              <a href="#">Gamini Dissanayake</a>.
              <br>
              <em><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2016
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/7759049">Paper</a>
              <p></p>
              <p>
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/16_traf_result.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Design of Highly Reliable Infrastructural Traffic Monitoring Using Laser and Vision Sensors</papertitle>
              </a>
              <br> 
              <strong>Hangxin Liu</strong>,
              <a href="#">Yi Tian</a>,
              <a href="#">Tomonari Furukawa</a>.
              <br>
              <em>ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference (ASME IDETC)</em>, 2016
              <br>
              <a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2016/V003T01A013/257048">Paper</a>
              <p></p>
              <p>
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/16_nfov_rbe.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Recursive Bayesian Estimation of NFOV Target using Diffraction and Reflection Signals</papertitle>
              </a>
              <br> 
              <a href="#">Kuya Takami</a>,
              <strong>Hangxin Liu</strong>,
              <a href="#">Tomonari Furukawa</a>,
              <a href="#">Makoto Kumon</a>,
              <a href="#">Gamini Dissanayake</a>.
              <br>
              <em>ISIF International Conference on Information Fusion (FUSION)</em>, 2016
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/7528118">Paper</a>
              <p></p>
              <p>
              </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <br>
              <p style="text-align:right;font-size:small;">
                Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
